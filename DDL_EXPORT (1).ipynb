{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "oqoz2l7c23bclgmbmr24",
   "authorId": "436807456974",
   "authorName": "SMORRIS_SFC",
   "authorEmail": "sean.morris@snowflake.com",
   "sessionId": "cd4dc71e-6a7c-4492-bf66-1e8a810876c7",
   "lastEditTime": 1761078233879
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "fe97e0af-7563-4e68-a76e-44271792731b",
   "metadata": {
    "language": "python",
    "name": "SELECT_DATABASE"
   },
   "outputs": [],
   "source": "database_name = ''",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "SET_CONTEXT"
   },
   "source": "# Import python packages\nimport streamlit as st\nimport pandas as pd\n\n# We can also use Snowpark for our analyses!\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "a472e61c-26a9-4c99-954e-4f1f5674857f",
   "metadata": {
    "language": "python",
    "name": "CREATE_TABLE"
   },
   "outputs": [],
   "source": "session.sql(f\"\"\"\n    CREATE OR REPLACE TABLE {database_name}.PUBLIC.DDL_EXPORT(\n        OBJECT_NAME STRING,\n        DDL STRING\n    );\n\"\"\").collect()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9d7d44a3-08d6-4629-a191-aa52d18a0aee",
   "metadata": {
    "language": "python",
    "name": "SELECT_TABLES"
   },
   "outputs": [],
   "source": "table_df = session.sql(f\"\"\"\n    SELECT TABLE_CATALOG||'.'||TABLE_SCHEMA||'.'||TABLE_NAME AS OBJECT_NAME\n    FROM {database_name}.INFORMATION_SCHEMA.TABLES\n    WHERE TABLE_SCHEMA NOT IN ('INFORMATION_SCHEMA','PUBLIC','SNOWFLAKE_TRACKER');\n\"\"\").to_pandas()\n\nprint(table_df['OBJECT_NAME'])",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "997f36b9-5f4f-4e03-b4fd-7a56d8e942f1",
   "metadata": {
    "language": "python",
    "name": "OUTPUT_CSV"
   },
   "outputs": [],
   "source": "import pandas as pd\n\n# Collect DDLs\nobject_names = []\nddls = []\n\nfor table_name in table_df['OBJECT_NAME']:\n    try:\n        ddl_result = session.sql(\n            \"SELECT GET_DDL('TABLE', ?) AS DDL\",\n            params=[table_name]\n        ).collect()\n        \n        object_names.append(table_name)\n        ddls.append(ddl_result[0]['DDL'])\n        \n    except Exception as e:\n        print(f\"âœ— {table_name}: {e}\")\n\n# Create DataFrame\nresult_df = pd.DataFrame({\n    'OBJECT_NAME': object_names,\n    'DDL': ddls\n})\n\n# Write to local CSV first\nlocal_file = '/tmp/ddl_export.csv'\nresult_df.to_csv(local_file, index=False, quoting=1)  # quoting=1 quotes all fields\n\nsession.sql(\"\"\"\n    CREATE OR REPLACE STAGE DDL_EXPORT_STAGE\n        FILE_FORMAT = (\n            TYPE = 'CSV'\n            FIELD_DELIMITER = ','\n            SKIP_HEADER = 1\n            FIELD_OPTIONALLY_ENCLOSED_BY = '\"'\n            NULL_IF = ('NULL', 'null', '')\n            EMPTY_FIELD_AS_NULL = TRUE\n            COMPRESSION = 'AUTO'\n        );\n\"\"\").collect()\n\nsession.file.put(local_file, \"@DDL_EXPORT_STAGE\", auto_compress=False, overwrite=True)",
   "execution_count": null
  }
 ]
}